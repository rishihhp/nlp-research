As is evident from the data and the graphs, the data does not fit a strictly linear behavior, but the assumption of
linearity due to the program being of O (N) runtime is accredited the non-linearity to heterogeneous data. In the field
of computer science, O (N) refers to the runtime complexity of an algorithm, and in this case, specifically the
stemming of the words, the bag of words, and the data processing.
O (N) indicates that the runtime of the algorithm grows approximately linearly with the size of the input. O (N) is a
worst-case runtime complexity (upper-bound), and some external factors in the code through nested loops or
recursive calls obviously have some other unforeseen impact on the runtime. A limitation of the present study was
its relatively small sample size of both computers to use and intents.json sizes.
Attempting to generalize these findings, the program and thus study design had to opt for more utilized Python
models. Hence, larger and more varied samples for future studies would be very beneficial, and it would also be
productive to see if CPU and GPU overclocking, upgrading, or downgrading have any effect on the timing.
Sadly, the only cross-validation technique used was running the simulation multiple times, which means there is a
caveat in the breadth and depth of the data utilized to answer and confirm the hypothesis. Same-sized files also
might perform differently depending on contextual data - which was not studied in this research, which potentially
botches the data. Quality issues, natural language processing bias, and computational constraints plagued this study,
somewhat limiting the scope. However, this study can stand on its own to serve as a baseline for natural language
process development. These results add to the body of knowledge of algorithm runtime for artificial intelligence and
highlight how machine learning algorithms run.
