Algorithms are paramount to any digital instrument. With the contemporary deluge of chatbots - with over 1.4
billion users worldwide - how humans interact with technology has been revolutionized [1]. While extensive
companies with considerable resources frequently develop these chatbots, aspiring computer scientists have begun
to dabble in assembling their chatbot systems.
Although a plethora of aspiring teens and adults run into issues compiling neural networks and consequently turn to
pre-made Python modules that make compiling neural networks much faster.
Python offers prevalent and efficient machine learning models, with Tensor Flow as the industry standard [2].
However, for academic purposes, NLTK, a natural language processing module, is a suitable alternative for neural
network development [3].
Similar to neural network development, there are artificial intelligence and machine learning. Although artificial
intelligence and machine learning are used interchangeably, some distinct differences exist.
Artificial intelligence refers to the broader way of viewing a “simulation” of human intelligence onto computers. It
utilizes various techniques: computer vision, natural language processing models, machine learning, etc. In short, it
tries to emulate human intelligence in order to interact with their environment through the usage of algorithms.
Machine learning is a subset of artificial intelligence that deals with the development of both algorithms and models
that allow computers to make predictions and decisions based on those predictions. Once the model is trained, it can
be used for the entirety of the program without fail or explicit programming. They are dynamic systems: improving
their performance through data and processing.
Like machine learning, natural language processing models are another subset of artificial intelligence. This subset
encompasses computers being able to “speak” like a human: interpretation and generation of the human language. It
combines techniques from linguistics and artificial intelligence to process and analyze text data in order to
commonly create question-answering systems such as chatbots [4].
An algorithm is a step-by-step set of instructions designed to solve specific problems. In the case of this study, it is
used to process and analyze data before making predictions based on the given data. Given the increased importance
of chatbots in the real world with new technologies such as Chat GPT and slews of innovation within the field, this
paper will study the relationship between NLTK (Natural Language Processing Module) and Tensor Flow
algorithms based on the data that it processes.
This study investigates the relationship between intents.json length and the Tensor Flow/NLTK runtime in order to
create a baseline for the efficiency of chatbot information creation. An intents.json file is utilized in the context of
NLP systems in the field of virtual assistants. It is a configuration file that defines the various intents/ purposes
behind user inputs or queries that the system can respond to. Each intent in the file is defined as an object with
specific properties. Specifically, it has a unique identifier, examples (training data), actions, parameters, and
contextual information. It is used in conjunction with dialogue management systems to build conversational AI
systems in order to respond to user inputs more effectively. An “intents.json” is simply the standard naming
convention.
The main significance of this paper lies in the fact that it can provide insights into the runtime required for training
chatbot models based on machine learning. This can provide organizations with the ability to properly allocate their
resources (processing, power, memory, money, etc.) based on the size of the data that they are processing. This can
help optimize their infrastructure and improve their resource management for testing chatbot development. This
research can serve as a baseline to help provide guidance on potential bottlenecks in the runtime of chatbot
implementations - aspiring to enable more robust and efficient chatbot systems.
